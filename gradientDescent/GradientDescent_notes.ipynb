{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Learning Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture below shows a simple, one dimentional, example of gradient descent:\n",
    "\n",
    "![demo](./0_fU8XFt-NCMZGAWND.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Deep learning models learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the thought process of how DL usually works: \n",
    "1. Guess a solution\n",
    "2. Compute the error (mistakes)\n",
    "3. Learn from mistakes and modify the parameters so that the model can make better guess about the solution\n",
    "\n",
    "DL models implement this process in mathematical language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need a mathematical description of the error landscape of the problem, then we need a way to find the minimum point of the landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few steps for the algorithm:\n",
    "1. Initialize random guess of minimum\n",
    "2. Loop over training iterations\n",
    "    * Compute derivative at guessed minimum\n",
    "    * Updated guess minimum is itself minus derivative scaled (timed) by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local vs. Global Minimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minima and global minima are concepts related to optimization in machine learning, particularly in the context of gradient descent:\n",
    "\n",
    "1. Local minima:\n",
    "    * Refers to the minima calculated using a **subset** of the training data or a single data point.\n",
    "    * In stochastic gradient descent, the local minima is computed for each training example or mini-batch.\n",
    "    * May point in a direction slightly different from the gradient of the entire dataset.\n",
    "\n",
    "2. Global minima:\n",
    "    * Represents the minima calculated using the entire training dataset.\n",
    "    * In batch gradient descent, the global minima is computed using all training examples.\n",
    "    * Provides a more accurate direction towards the overall minimum of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Local Gradient Good or Bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outset, local minima can cause the model be trapped in a less optimal solution. However, the success of deep learning (in spite of the problems) is obvious - the reason remains mystery.\n",
    "\n",
    "It is possible that there can be many good solutions, or it may be extremely likely to have local minima in high-dimensional space, where humans has no ability to visually determine if a local minima exists. \n",
    "\n",
    "* in machine learning we can have hundreds or even thousands of dimensions. \n",
    "\n",
    "In a high dimensional space, Gradient Descend must reach a local minima that is true for all dimensions, which is very unlikely. And this can be why local minima has not been a big problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
